\documentclass[12pt, a4paper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{float}
\usepackage{authblk}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{mathptmx}

% Graphics path
\graphicspath{{../notebooks/models/figures/}{../notebooks/models/}}

% Title formatting
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection.}{0.5em}{}

% Line spacing
\onehalfspacing

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{\textbf{Brain Tumor Classification from MRI Images: A Comparative Study of CNN Architectures}}

\author[1]{Your Name}
\affil[1]{NAAMII, Kathmandu, Nepal \\ \texttt{email@example.com}}

\date{}

\maketitle

\begin{abstract}
Brain tumor classification from MRI images is a critical task in medical imaging that can assist radiologists in early diagnosis. In this work, we present a comparative study of three convolutional neural network architectures for classifying brain tumors into four categories: glioma, meningioma, pituitary tumor, and no tumor. We evaluate a custom baseline CNN, ResNet-18 trained from scratch, and a pretrained DenseNet-121 fine-tuned on the BRISC 2025 dataset. Our experiments demonstrate that ResNet-18 trained from scratch significantly outperforms both the pretrained transfer learning approach and the custom baseline architecture. We further analyze model behavior using Grad-CAM visualizations to interpret the learned features. The results show that ResNet-18 achieves the highest accuracy of 92.1\% with an F1-score of 91.9\%, while DenseNet-121 and Baseline CNN achieve 79.6\% and 75.7\% respectively. Notably, DenseNet-121's underperformance is attributed to training instability caused by small batch sizes (16) necessitated by GPU memory constraints when processing high-resolution images, combined with the limited dataset size, which resulted in noisy gradient estimates and prevented effective utilization of ImageNet pretraining. These findings provide insights into the practical challenges of deploying complex architectures under resource-constrained conditions for medical image classification.
\end{abstract}

\noindent\textbf{Keywords:} Brain tumor classification, Convolutional Neural Networks, Transfer Learning, Medical Image Analysis, Grad-CAM, Deep Learning

\vspace{1em}%==============================================================================
\section{Introduction}
%==============================================================================

% TODO: Write introduction
% - Background on brain tumor classification
% - Importance of automated diagnosis
% - Brief overview of deep learning in medical imaging
% - Contributions of this work

Brain tumors are among the most serious forms of cancer, requiring early and accurate diagnosis for effective treatment planning. Magnetic Resonance Imaging (MRI) is the primary modality for brain tumor detection due to its superior soft tissue contrast. However, manual interpretation of MRI scans is time-consuming and subject to inter-observer variability. Deep learning, particularly Convolutional Neural Networks (CNNs), has shown remarkable success in medical image classification tasks. Transfer learning from large-scale datasets like ImageNet has further improved performance on medical imaging tasks where labeled data is limited.

In this work, we present a comparative study of three CNN architectures for brain tumor classification:
\begin{itemize}
  \item A custom baseline CNN designed specifically for this task
  \item ResNet-18 trained from scratch
  \item DenseNet-121 pretrained on ImageNet and fine-tuned
\end{itemize}

Our main contributions include:
\begin{enumerate}
  \item Comprehensive comparison of different CNN architectures on brain tumor classification
  \item Analysis of training strategies showing that ResNet-18 from scratch outperforms pretrained transfer learning, with investigation into the training instability and resource constraints that limited DenseNet-121 performance
  \item Interpretation of model decisions using Grad-CAM visualizations to understand tumor-specific feature learning
  \item Quantitative evaluation demonstrating 92.1\% accuracy with ResNet-18, significantly reducing error rate to 7.9\%
  \item Practical insights into the impact of batch size, dataset size, and architectural complexity on training stability under resource-constrained conditions
\end{enumerate}

%==============================================================================
\section{Dataset}
%==============================================================================

\subsection{Dataset Description}

We use the BRISC 2025 brain tumor MRI dataset \cite{fateh2025briscannotateddatasetbrain} for our experiments. The dataset comprises 6,000 grayscale MRI images organized into four classes, with a predefined split of 5,000 images for training and 1,000 images for testing. The images show various orientations (axial, sagittal, coronal) and are acquired using T1-weighted MRI sequences. The dataset exhibits a moderate class imbalance, with pituitary tumors being the most represented class and no tumor cases being the least represented. Image dimensions vary, with a majority being 512×512 pixels, though some images range from 202×369 to 1275×1427 pixels.

\begin{itemize}
  \item \textbf{Glioma}: 1,147 images (22.9\%)
  \item \textbf{Meningioma}: 1,329 images (26.6\%)
  \item \textbf{Pituitary}: 1,457 images (29.1\%)
  \item \textbf{No Tumor}: 1,067 images (21.3\%)
\end{itemize}

\begin{table}[H]
  \centering
  \caption{Dataset Statistics and Class Distribution}
  \label{tab:dataset}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Class} & \textbf{Count} & \textbf{Percentage} & \textbf{Class ID} & \textbf{Characteristics}  \\
    \midrule
    Glioma         & 1,147          & 22.9\%              & 0                 & Infiltrative brain tumors \\
    Meningioma     & 1,329          & 26.6\%              & 1                 & Meningeal layer tumors    \\
    No Tumor       & 1,067          & 21.3\%              & 2                 & Healthy brain scans       \\
    Pituitary      & 1,457          & 29.1\%              & 3                 & Pituitary gland tumors    \\
    \midrule
    \textbf{Train} & 5,000          & 83.3\%              & -                 & Training set              \\
    \textbf{Test}  & 1,000          & 16.7\%              & -                 & Test set                  \\
    \midrule
    \textbf{Total} & 6,000          & 100\%               & -                 & -                         \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Image Properties Analysis (based on 100 random samples)}
  \label{tab:image_properties}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Property} & \textbf{Mean} & \textbf{Std} & \textbf{Min} & \textbf{Max} \\
    \midrule
    Height (pixels)   & 498.0         & 107.3        & 369          & 1,427        \\
    Width (pixels)    & 480.0         & 123.7        & 202          & 1,275        \\
    Mean Intensity    & 42.4          & 13.8         & 19.0         & 86.0         \\
    Std Intensity     & 44.4          & 9.8          & 28.5         & 82.7         \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Dataset Visualization}

Figure \ref{fig:class_distribution} illustrates the class distribution in the training dataset, showing that pituitary tumors are the most represented class (29.1\%) and no tumor cases are the least represented (21.3\%).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{samples/class_distribution.pdf}
  \caption{Class distribution in the BRISC 2025 training dataset showing both absolute counts and percentage breakdown across the four tumor classes.}
  \label{fig:class_distribution}
\end{figure}

Figure \ref{fig:dataset_samples} shows representative samples from each class in the BRISC 2025 dataset. The dataset exhibits significant variation in imaging planes (axial, sagittal, coronal) and contrast levels across different tumor types.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.95\textwidth]{samples/all_tumor_samples.pdf}
%   \caption{Sample MRI images from each class in the BRISC 2025 dataset, showing the diversity of imaging orientations and tumor appearances.}
%   \label{fig:dataset_samples}
% \end{figure}

\begin{figure}[H]
  \centering
  \subfloat[Glioma samples]{\includegraphics[width=0.48\textwidth]{samples/glioma_samples.pdf}\label{fig:glioma_samples}}
  \hfill
  \subfloat[Meningioma samples]{\includegraphics[width=0.48\textwidth]{samples/meningioma_samples.pdf}\label{fig:meningioma_samples}}
  \caption{Sample images from (a) Glioma and (b) Meningioma classes showing characteristic tumor patterns.}
  \label{fig:tumor_samples_1}
\end{figure}

\begin{figure}[H]
  \centering
  \subfloat[Pituitary samples]{\includegraphics[width=0.48\textwidth]{samples/pituitary_samples.pdf}\label{fig:pituitary_samples}}
  \hfill
  \subfloat[No Tumor samples]{\includegraphics[width=0.48\textwidth]{samples/no_tumor_samples.pdf}\label{fig:no_tumor_samples}}
  \caption{Sample images from (a) Pituitary tumor and (b) No Tumor classes.}
  \label{fig:tumor_samples_2}
\end{figure}

\subsection{Data Preprocessing}

As shown in Table \ref{tab:image_properties}, the dataset contains images with varying dimensions and intensity distributions. To ensure consistency and improve model training, the following preprocessing pipeline was applied:

\begin{enumerate}
  \item \textbf{Image Loading}: Load grayscale MRI images in their original dimensions
  \item \textbf{Resizing}: Standardize all images to 512×512 pixels using bilinear interpolation
  \item \textbf{Normalization}: Scale pixel values to [0, 1] range and normalize with mean=0.5 and std=0.5
  \item \textbf{Data Augmentation} (training only):
        \begin{itemize}
          \item Random horizontal flip (p=0.5)
          \item Random rotation (±15°)
          \item Random affine transformations (translate, scale)
          \item Color jitter (brightness, contrast adjustments)
        \end{itemize}
\end{enumerate}

The class imbalance shown in Table \ref{tab:dataset} was addressed using weighted cross-entropy loss during training, with weights inversely proportional to class frequencies.

%==============================================================================
\section{Methodology}
%==============================================================================

\subsection{Model Architectures}

\subsubsection{Baseline CNN}

Our custom baseline CNN architecture consists of five convolutional blocks with progressively increasing filters (32→64→128→256→512). Each block contains:
\begin{itemize}
  \item 3×3 convolutional layer
  \item Batch normalization
  \item ReLU activation
  \item 2×2 max pooling
  \item Dropout (progressive: 0.1→0.5)
\end{itemize}

The feature extractor is followed by global average pooling and three fully connected layers with dropout for regularization.

% Architecture diagram placeholder
\begin{figure}[H]
  \centering
  \fbox{\parbox{0.8\textwidth}{\centering\vspace{3cm}[PLACEHOLDER: Baseline CNN Architecture Diagram]\vspace{3cm}}}
  \caption{Architecture of the Baseline CNN model.}
  \label{fig:baseline_arch}
\end{figure}

\subsubsection{ResNet-18 from Scratch}

We implemented ResNet-18 \cite{he2015deepresiduallearningimage} with the following characteristics:
\begin{itemize}
  \item Initial 7×7 convolution with 64 filters
  \item Four residual layers with [2, 2, 2, 2] basic blocks
  \item Skip connections to address vanishing gradient problem
  \item Modified input layer for single-channel grayscale images
\end{itemize}

\subsubsection{Pretrained DenseNet-121}

DenseNet-121 \cite{huang2018denselyconnectedconvolutionalnetworks} with ImageNet pretrained weights was adapted for our task:
\begin{itemize}
  \item Modified first convolutional layer for grayscale input (1 channel)
  \item Frozen backbone layers during initial training
  \item Custom classifier head with 4 output classes
  \item Fine-tuned on brain tumor data
\end{itemize}

\subsection{Training Configuration}

% TODO: Verify and update training parameters
\begin{table}[h]
  \centering
  \caption{Training Hyperparameters}
  \label{tab:hyperparams}
  \begin{tabular}{ll}
    \toprule
    \textbf{Parameter} & \textbf{Value}            \\
    \midrule
    Epochs             & 30                        \\
    Learning Rate      & 0.0005                    \\
    Batch Size         & 16                        \\
    Optimizer          & Adam                      \\
    Loss Function      & Weighted CrossEntropyLoss \\
    LR Scheduler       & ReduceLROnPlateau         \\
    Weight Decay       & [PLACEHOLDER]             \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Class Imbalance Handling}

To address class imbalance, we employed weighted cross-entropy loss:
\begin{equation}
  \mathcal{L} = -\sum_{i=1}^{C} w_i \cdot y_i \cdot \log(\hat{y}_i)
\end{equation}
where $w_i$ is the weight for class $i$, computed inversely proportional to class frequency.

%==============================================================================
\section{Evaluation Metrics}
%==============================================================================

We evaluate model performance using the following metrics:

\textbf{Accuracy}: Overall classification accuracy
\begin{equation}
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\textbf{Precision}: Ratio of true positives to predicted positives
\begin{equation}
  \text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\textbf{Recall (Sensitivity)}: Ratio of true positives to actual positives
\begin{equation}
  \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\textbf{F1-Score}: Harmonic mean of precision and recall
\begin{equation}
  \text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

All metrics are computed using weighted averaging to account for class imbalance.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Training Curves}

The training progress of each model is shown in the following figures, displaying the accuracy and loss curves over epochs.

\begin{figure}[H]
  \centering
  \subfloat[Accuracy curve]{\includegraphics[width=0.48\textwidth]{baseline_cnn/accuracy_curve.png}\label{fig:baseline_acc}}
  \hfill
  \subfloat[Loss curve]{\includegraphics[width=0.48\textwidth]{baseline_cnn/loss_curve.png}\label{fig:baseline_loss}}
  \caption{Training curves for Baseline CNN showing (a) accuracy and (b) loss progression over epochs.}
  \label{fig:training_baseline}
\end{figure}

\begin{figure}[H]
  \centering
  \subfloat[Accuracy curve]{\includegraphics[width=0.48\textwidth]{resnet18/accuracy_curve.png}\label{fig:resnet_acc}}
  \hfill
  \subfloat[Loss curve]{\includegraphics[width=0.48\textwidth]{resnet18/loss_curve.png}\label{fig:resnet_loss}}
  \caption{Training curves for ResNet-18 showing (a) accuracy and (b) loss progression over epochs.}
  \label{fig:training_resnet}
\end{figure}

\begin{figure}[H]
  \centering
  \subfloat[Accuracy curve]{\includegraphics[width=0.48\textwidth]{pretrained_densenet121/accuracy_curve.png}\label{fig:densenet_acc}}
  \hfill
  \subfloat[Loss curve]{\includegraphics[width=0.48\textwidth]{pretrained_densenet121/loss_curve.png}\label{fig:densenet_loss}}
  \caption{Training curves for Pretrained DenseNet-121 showing (a) accuracy and (b) loss progression over epochs.}
  \label{fig:training_densenet}
\end{figure}

\subsection{Overall Performance Comparison}

% Model comparison figure
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{metrics/performance_comparison.pdf}
  \caption{Performance comparison of all three models across different metrics.}
  \label{fig:model_comparison}
\end{figure}

\begin{table}[h]
  \centering
  \caption{Model Performance Comparison}
  \label{tab:results}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\\\
    \midrule
    Baseline CNN   & 75.7\%            & 77.6\%             & 75.7\%          & 74.7\%            \\\\
    ResNet-18      & 92.1\%            & 92.6\%             & 92.1\%          & 91.9\%            \\\\
    DenseNet-121   & 79.6\%            & 82.0\%             & 79.6\%          & 79.5\%            \\\\
    \bottomrule
  \end{tabular}
\end{table}

Table \ref{tab:results} presents the comprehensive performance metrics for all three models on the test set of 1,000 images. ResNet-18 emerges as the best performing architecture, achieving 92.1\% accuracy and correctly classifying 921 out of 1,000 test images, with only 79 misclassifications (7.9\% error rate). This represents a substantial improvement over DenseNet-121 (204 misclassifications, 20.4\% error rate) and Baseline CNN (243 misclassifications, 24.3\% error rate). The consistent performance across all metrics (precision, recall, and F1-score) indicates that ResNet-18 provides robust and balanced classification across all tumor classes.

\subsection{Classification Metrics}

% ROC Curves
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{metrics/roc_curves.pdf}
  \caption{ROC curves for all three models showing true positive rate vs false positive rate.}
  \label{fig:roc_curves}
\end{figure}

% PR Curves
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{metrics/pr_curves.pdf}
  \caption{Precision-Recall curves for all three models.}
  \label{fig:pr_curves}
\end{figure}

\subsection{Confusion Matrices}

% Confusion matrices
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{metrics/confusion_matrices.pdf}
  \caption{Confusion matrices for all three models showing per-class predictions.}
  \label{fig:confusion_matrices}
\end{figure}

\subsection{Per-Class Performance}

% TODO: Add per-class metrics table
\begin{table}[h]
  \centering
  \caption{Per-Class F1-Scores}
  \label{tab:perclass}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Glioma} & \textbf{Meningioma} & \textbf{Pituitary} & \textbf{No Tumor} \\
    \midrule
    Baseline CNN   & [XX.X\%]        & [XX.X\%]            & [XX.X\%]           & [XX.X\%]          \\
    ResNet-18      & [XX.X\%]        & [XX.X\%]            & [XX.X\%]           & [XX.X\%]          \\
    DenseNet-121   & [XX.X\%]        & [XX.X\%]            & [XX.X\%]           & [XX.X\%]          \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Prediction Examples}

% Prediction examples for all models
\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{predictions/baselinecnn.pdf}
  \caption{Sample predictions from Baseline CNN.}
  \label{fig:predictions_baseline}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{predictions/resnet18.pdf}
  \caption{Sample predictions from ResNet-18.}
  \label{fig:predictions_resnet}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{predictions/pretraineddensenet121.pdf}
  \caption{Sample predictions from Pretrained DenseNet-121.}
  \label{fig:predictions_densenet}
\end{figure}

%==============================================================================
\section{Model Interpretability}
%==============================================================================

\subsection{Grad-CAM Visualization}

To understand what regions of the MRI images the models focus on for classification, we employ Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{selvaraju2017grad}. Grad-CAM produces visual explanations by using the gradients flowing into the final convolutional layer.

\subsubsection{Correctly Classified Samples}

% Grad-CAM visualizations for all models
\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{gradcam/baselinecnn.pdf}
  \caption{Grad-CAM visualizations for correctly classified samples - Baseline CNN.}
  \label{fig:gradcam_baseline}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{gradcam/resnet18.pdf}
  \caption{Grad-CAM visualizations for correctly classified samples - ResNet-18.}
  \label{fig:gradcam_resnet}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{gradcam/pretraineddensenet121.pdf}
  \caption{Grad-CAM visualizations for correctly classified samples - Pretrained DenseNet-121.}
  \label{fig:gradcam_densenet}
\end{figure}

\subsubsection{Misclassified Samples}

% Grad-CAM for misclassified samples
\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{gradcam_misclassified/baselinecnn.pdf}
  \caption{Grad-CAM for misclassified samples - Baseline CNN.}
  \label{fig:gradcam_misclassified_baseline}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{gradcam_misclassified/resnet18.pdf}
  \caption{Grad-CAM for misclassified samples - ResNet-18.}
  \label{fig:gradcam_misclassified_resnet}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{gradcam_misclassified/pretraineddensenet121.pdf}
  \caption{Grad-CAM for misclassified samples - Pretrained DenseNet-121.}
  \label{fig:gradcam_misclassified_densenet}
\end{figure}

\subsection{Analysis of Model Attention}

% TODO: Write analysis based on Grad-CAM results
\begin{itemize}
  \item \textbf{Baseline CNN}: [PLACEHOLDER: Observations about what regions the baseline model focuses on]
  \item \textbf{ResNet-18}: [PLACEHOLDER: Observations about ResNet attention patterns]
  \item \textbf{DenseNet-121}: [PLACEHOLDER: Observations about DenseNet attention patterns, likely more focused due to pretraining]
\end{itemize}

\subsection{Error Analysis}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{misclassified/baselinecnn.pdf}
  \caption{Misclassified samples from Baseline CNN.}
  \label{fig:misclassified_baseline}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{misclassified/resnet18.pdf}
  \caption{Misclassified samples from ResNet-18.}
  \label{fig:misclassified_resnet}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{misclassified/pretraineddensenet121.pdf}
  \caption{Misclassified samples from Pretrained DenseNet-121.}
  \label{fig:misclassified_densenet}
\end{figure}

\subsection{Feature Map Visualization}

To understand the hierarchical feature learning in each model, we visualize the activation maps from different convolutional layers.

% Feature maps visualizations
\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{feature_maps/baselinecnn.pdf}
  \caption{Feature maps from Baseline CNN layers.}
  \label{fig:feature_maps_baseline}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{feature_maps/resnet18.pdf}
  \caption{Feature maps from ResNet-18 layers.}
  \label{fig:feature_maps_resnet}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{feature_maps/pretraineddensenet121.pdf}
  \caption{Feature maps from DenseNet-121 layers.}
  \label{fig:feature_maps_densenet}
\end{figure}

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Model Comparison Analysis}

Our experimental results reveal several important insights about the effectiveness of different architectures for brain tumor classification. ResNet-18 trained from scratch significantly outperformed both the pretrained DenseNet-121 and the custom Baseline CNN, achieving 92.1\% accuracy compared to 79.6\% and 75.7\% respectively.

Some of the key observations from comparision of three models are:
\begin{itemize}
  \item \textbf{Training from Scratch vs Transfer Learning}: Contrary to common expectations in medical imaging, ResNet-18 trained from scratch outperformed the pretrained DenseNet-121 by 12.5 percentage points. This performance gap can be attributed to multiple factors: (1) learning features directly from MRI data proved more effective than adapting ImageNet features trained on natural images, which may not translate optimally to grayscale medical imaging patterns, and (2) DenseNet-121 experienced significant training instability due to resource constraints. The small batch size of 16, necessitated by GPU memory limitations when processing 512×512 high-resolution images through the deeper DenseNet architecture, resulted in noisy gradient estimates. Combined with the relatively limited dataset of 5,000 training images, these factors created an unstable optimization landscape that prevented DenseNet-121 from effectively leveraging its ImageNet pretraining, as evidenced by dramatic validation loss spikes throughout training.

  \item \textbf{Architecture Depth and Residual Connections}: ResNet-18's superior performance can be attributed to its residual connections that facilitate gradient flow and enable effective training of deeper networks. The skip connections allow the model to learn both low-level texture features and high-level semantic features of different tumor types. The Baseline CNN, lacking these architectural advantages, achieved only 75.7\% accuracy despite having progressive feature extraction layers.

  \item \textbf{Error Rate Reduction}: ResNet-18 achieved a remarkable 7.9\% error rate, reducing misclassifications by more than half compared to DenseNet-121 (20.4\%) and nearly two-thirds compared to Baseline CNN (24.3\%). This translates to 125 fewer errors than DenseNet-121 and 164 fewer errors than Baseline CNN on the 1,000-image test set, demonstrating significant clinical relevance.
\end{itemize}

\subsection{Interpretability Insights}

Our Grad-CAM visualizations provide crucial insights into how each model makes classification decisions:
\begin{itemize}
  \item \textbf{Tumor-Specific Attention}: Grad-CAM heatmaps reveal that ResNet-18 consistently focuses on anatomically relevant regions corresponding to tumor locations. For glioma cases, the model attends to infiltrative patterns in brain tissue, while for meningioma, it focuses on well-defined boundaries near meningeal layers. This demonstrates that the model has learned clinically meaningful features.

  \item \textbf{Error Analysis Patterns}: Examination of misclassified samples shows that errors often occur in cases with subtle tumor boundaries or unusual imaging orientations. The Grad-CAM visualizations for misclassified samples indicate that when models fail, they tend to focus on non-discriminative background regions rather than tumor-specific features, suggesting that edge cases remain challenging.

  \item \textbf{Model Confidence and Reliability}: Feature map visualizations across different layers show that ResNet-18 develops hierarchical representations, with early layers detecting edges and textures, and deeper layers capturing tumor-specific patterns. This progressive feature learning contributes to the model's robust performance and generalization capability.
\end{itemize}

\subsection{Limitations}

While our study demonstrates strong performance, several limitations should be acknowledged:
\begin{itemize}
  \item \textbf{Dataset Size and Diversity}: With 5,000 training images from the BRISC 2025 dataset, the models may not capture the full variability of tumor presentations across different imaging protocols, scanner manufacturers, and patient populations. Larger multi-center datasets could improve generalization.

  \item \textbf{Class Imbalance}: The dataset exhibits moderate class imbalance (21.3\%-29.1\% distribution), which we addressed using weighted loss functions. However, minority classes may still be underrepresented, potentially affecting per-class performance.

  \item \textbf{Single MRI Sequence}: The dataset contains only T1-weighted MRI sequences. Clinical diagnosis typically utilizes multiple MRI sequences (T1, T2, FLAIR) which provide complementary information. Future work should incorporate multi-sequence data.

  \item \textbf{Binary Nature of Classification}: Our model provides class predictions without uncertainty quantification. Implementing probabilistic outputs or ensemble methods could provide confidence intervals useful for clinical decision support.
\end{itemize}

%==============================================================================
\section{Additional Findings}
%==============================================================================

Our experiments revealed several noteworthy observations beyond the primary results:

\begin{itemize}
  \item \textbf{Training Stability}: ResNet-18 demonstrated the most stable training progression, reaching peak validation accuracy of 95.9\% at epoch 27 before slight overfitting. This suggests that early stopping or additional regularization could potentially push performance even higher.

  \item \textbf{DenseNet-121 Training Instability}: The pretrained DenseNet-121 exhibited significant training instability, with validation loss showing dramatic spikes at epochs 6, 10, and 18, ultimately achieving only 79.6\% accuracy. This behavior can be attributed to several factors related to the limited computational resources and dataset characteristics. The small batch size of 16, necessitated by GPU memory constraints when processing high-resolution 512×512 MRI images through the deeper DenseNet-121 architecture, resulted in noisy gradient estimates during optimization. Additionally, the relatively small dataset of 5,000 training images combined with the small validation set made the validation metrics highly sensitive to individual misclassifications, causing large fluctuations in loss values. These training dynamics were further complicated by DenseNet's use of dropout layers, which disabled neurons during training but activated the full network capacity during validation, occasionally causing validation loss to drop below training loss in later epochs. The combination of small batch size, limited dataset size, and architectural complexity created an unstable optimization landscape that prevented DenseNet-121 from effectively leveraging its ImageNet pretraining, ultimately making it less accurate than the simpler ResNet-18 architecture.

  \item \textbf{Training Efficiency}: ResNet-18 required 30 epochs to converge, with the best model selected at epoch 27. The training curves show consistent improvement without severe overfitting, indicating that the architecture and hyperparameters were well-suited to the task and that ResNet-18's simpler architecture was more robust to the constraints of small batch size and limited dataset size.

  \item \textbf{Computational Considerations}: Despite its superior performance, ResNet-18 maintained reasonable computational requirements for training (30 epochs with batch size 16), making it practical for research and clinical deployment scenarios. The shallower architecture of ResNet-18 compared to DenseNet-121 resulted in more stable training dynamics under resource-constrained conditions.
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

In this work, we presented a comprehensive comparative study of three CNN architectures for brain tumor classification using the BRISC 2025 MRI dataset. Our experiments demonstrate that ResNet-18 trained from scratch significantly outperforms both pretrained transfer learning (DenseNet-121) and custom baseline approaches, achieving 92.1\% accuracy with 92.6\% precision and 91.9\% F1-score. With only 79 misclassifications out of 1,000 test images (7.9\% error rate), ResNet-18 provides reliable tumor classification that could assist radiologists in clinical workflows.

Contrary to conventional wisdom in medical imaging, the pretrained DenseNet-121 achieved only 79.6\% accuracy, underperforming the from-scratch trained ResNet-18 by 12.5 percentage points. This finding suggests that for specialized medical imaging tasks like brain tumor classification from grayscale MRI, domain-specific feature learning may be more effective than adapting features from natural image datasets like ImageNet.

Grad-CAM visualizations revealed that ResNet-18 learns to focus on anatomically relevant tumor regions, with attention patterns corresponding to expected tumor locations for different classes. This confirms that the models learn clinically meaningful features rather than spurious correlations.

Future work includes:
\begin{itemize}
  \item Incorporating multi-sequence MRI data (T1, T2, FLAIR) to leverage complementary information
  \item Expanding to larger multi-center datasets to improve generalization across different imaging protocols
  \item Implementing uncertainty quantification through ensemble methods or Bayesian approaches
  \item Exploring attention mechanisms and transformer architectures for medical image analysis
  \item Clinical validation studies to assess real-world deployment feasibility
\end{itemize}

%==============================================================================
% References
%==============================================================================
\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
